{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMnpISBLa76557oqd2sMz2T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pKNWLklP9Iu0","outputId":"f04c6f69-1c1b-4094-cf2d-4b4fbcc7f48b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]}],"source":["# CASE STUDY 04 – NLP: Sentiment Analysis (Deep Learning)\n","\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","# =====================\n","# Dataset (10 Sample Review)\n","# =====================\n","texts = [\n","    \"I love this movie, it was fantastic and thrilling!\",\n","    \"Terrible film, waste of time.\",\n","    \"Amazing acting and great story.\",\n","    \"Worst movie I’ve seen in years.\",\n","    \"Absolutely loved the characters and the plot.\",\n","    \"It was boring and too long.\",\n","    \"Brilliant performance by the lead actor.\",\n","    \"I didn’t enjoy the film at all.\",\n","    \"Highly recommended for everyone.\",\n","    \"Disappointing and poorly directed.\"\n","]\n","labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative\n","\n","# =====================\n","# Tokenizing & Padding\n","# =====================\n","max_words = 1000\n","tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","padded = pad_sequences(sequences, padding='post')\n","\n","# =====================\n","# Train Test Split\n","# =====================\n","X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n","\n","# =====================\n","# Build LSTM Model\n","# =====================\n","model = Sequential([\n","    Embedding(input_dim=max_words, output_dim=32, input_length=padded.shape[1]),\n","    SpatialDropout1D(0.2),\n","    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n","    Dense(1, activation='sigmoid')\n","])\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# =====================\n","# Train Model\n","# =====================\n","model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=2, verbose=1)\n","\n","# =====================\n","# Evaluate Model\n","# =====================\n","y_pred_probs = model.predict(X_test)\n","y_pred = (y_pred_probs > 0.5).astype(int)\n","\n","print(\"=== CLASSIFICATION REPORT ===\")\n","print(classification_report(y_test, y_pred))"]}]}